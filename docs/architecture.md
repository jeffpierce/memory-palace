# Architecture & Vision

## The Problem

Every AI session starts as a blank slate. Context windows are finite. Sessions end, knowledge dies. Each AI instance is an island.

Current solutions are all vendor-locked: ChatGPT's memory only works with OpenAI. Claude's projects only work with Anthropic. Switch providers and you start over. Your accumulated context â€” decisions, preferences, project history â€” belongs to the vendor, not to you.

Meanwhile, the industry races to build bigger context windows. 128K. 200K. 1M tokens. But a bigger scratchpad isn't memory. You don't solve human amnesia by giving someone a bigger whiteboard.

## The Solution

Memory Palace takes a different approach: **memory doesn't belong inside the model â€” it belongs alongside it.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Your AI Stack                   â”‚
â”‚                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚ Claude  â”‚  â”‚  Gemini â”‚  â”‚  Local  â”‚  ...    â”‚
â”‚   â”‚         â”‚  â”‚         â”‚  â”‚  Qwen  â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚
â”‚        â”‚            â”‚            â”‚               â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                     â”‚                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚              â”‚  MCP (open  â”‚                     â”‚
â”‚              â”‚  protocol)  â”‚                     â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                     â”‚                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚         â”‚   Memory Palace       â”‚                â”‚
â”‚         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                â”‚
â”‚         â”‚   â”‚ SQLite/Postgresâ”‚   â”‚                â”‚
â”‚         â”‚   â”‚ + Embeddings  â”‚   â”‚                â”‚
â”‚         â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                YOUR HARDWARE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Memory Palace is a persistent semantic memory layer that any MCP-compatible AI can access. It separates memory from the model, the same way databases separated data from applications decades ago.

The context window becomes **working memory** â€” the scratchpad for the current task. Memory Palace is **long-term storage** â€” the accumulated knowledge that persists across sessions, models, and providers.

That's how actual brains work. Short-term processing buffer plus long-term retrieval.

## What This Solves

### 1. Cross-Session Continuity

AI doesn't forget anymore. Sessions end, memory stays. Start a new conversation and recall what happened last week, last month, or last year.

### 2. Cross-Model Portability

Switch from Claude to Gemini to local Qwen? Same memories. Zero migration. **The model is replaceable, the memory isn't.**

### 3. Cross-Subscription Independence

Cancel Anthropic, sign up for OpenAI, spin up local Ollama â€” doesn't matter. Your memory layer doesn't care who's thinking, only who's remembering.

### 4. Zero Cloud Dependency

It runs on YOUR hardware. SQLite + local embeddings via Ollama. No one's training on your memories. No one's monetizing your context. No API keys to a memory service that'll sunset in 18 months.

### 5. No Vendor Lock-In

ChatGPT's memory locks you to OpenAI. Claude's project knowledge locks you to Anthropic. Gemini's context locks you to Google. Memory Palace? It's *yours*. The protocol is open. The data is local. Walk away from any provider whenever you want.

### 6. Multi-Instance Coordination

The handoff system means AI instances aren't just individually persistent â€” they can communicate. Your desktop AI can leave a note for your CLI agent. Your coding assistant can pass context to your chat assistant. That's not just memory â€” it's organizational infrastructure.

### 7. Data Sovereignty

Your memories, your conversations, your context â€” it's in a SQLite file on YOUR machine. Full stop. `SELECT * FROM memories` whenever you want. Export it. Back it up. Audit it. Try doing that with any cloud AI's memory system.

## The Knowledge Graph: Connected Memory

**Status:** âœ… Shipping

Semantic search finds memories by meaning. But memories don't exist in isolation â€” they relate to each other. A decision connects to the architecture it shaped, which connects to the incident that informed it, which connects to the policy that prevents recurrence.

Memory Palace includes a built-in knowledge graph with typed, directional, weighted edges:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  relates_to  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Auth Decision    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ JWT Architecture â”‚
â”‚ (decision)       â”‚              â”‚ (architecture)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                â”‚
    exemplifies                      caused_by
         â”‚                                â”‚
         â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token Expiry     â”‚              â”‚ Session Hijack   â”‚
â”‚ Incident         â”‚              â”‚ Incident         â”‚
â”‚ (event)          â”‚              â”‚ (event)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Three Levels of Memory

1. **Storage** (flat files) â€” things exist
2. **Search** (embeddings) â€” things are findable by meaning
3. **Understanding** (knowledge graph) â€” things are *connected*

### Why This Matters for Code

A codebase with 500 files doesn't fit in any context window. But a graph traversal at depth 2â€“3 from any starting node gives you exactly the relevant context â€” nothing more, nothing less:

```
memory_graph(start_id=PaymentService, max_depth=2)

â†’ PaymentService
  â”œâ”€â”€ uses â†’ OutboxPattern (architecture)
  â”‚   â””â”€â”€ publishes_to â†’ EventBus (architecture)
  â”œâ”€â”€ caused_by â†’ DuplicateChargeIncident (event)
  â”‚   â””â”€â”€ informed â†’ NeverCallEventBusDirectly (decision)
  â””â”€â”€ depends_on â†’ UserService (architecture)
      â””â”€â”€ authenticates_via â†’ JWTAuth (architecture)
```

The AI doesn't need to ingest 500 files. It traverses the graph, pulling only what's connected to the question being asked. Small context windows become a non-issue when you have a map of how everything relates.

**Known limitations:**
- Traversing from hub nodes (identity/foundational memories) at depth 2+ can return megabytes
- Needs result limits and degree-aware traversal strategies (not yet implemented)
- Embedding model truncates files >8192 tokens

### Graph Tools

| Tool | Description | Status |
|------|-------------|--------|
| `memory_link` | Create a typed, weighted, optionally bidirectional edge between two memories | âœ… Shipping |
| `memory_unlink` | Remove edges between memories | âœ… Shipping |
| `memory_related` | Get immediate connections (1 hop) from a memory | âœ… Shipping |
| `memory_graph` | Breadth-first traversal to configurable depth | âœ… Shipping |
| `memory_relationship_types` | List standard relationship types | âœ… Shipping |

Edges include metadata explaining *why* the connection exists, strength weights for traversal filtering, and directional semantics for accurate graph queries.

## The Handoff System: Decentralized Agent Coordination

**Status:** âœ… Shipping (polling-based)

### The Old Way: Hub-and-Spoke

Traditional agentic swarm architectures use a controller:

```
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Controller AI      â”‚
       â”‚  (big context,      â”‚
       â”‚   expensive,        â”‚
       â”‚   bottleneck)       â”‚
       â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚      â”‚      â”‚
       â”Œâ”€â”€â”´â”€â”€â”â”Œâ”€â”€â”´â”€â”€â”â”Œâ”€â”€â”´â”€â”€â”
       â”‚ W-A â”‚â”‚ W-B â”‚â”‚ W-C â”‚
       â””â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”˜
```

Everything funnels through the controller. Controller's context fills up. Controller becomes the single point of failure. Controller is the most expensive token burn in the whole system.

Hub-and-spoke doesn't scale. We've known this since distributed systems 101.

### The New Way: Shared Memory Bus

Memory Palace + handoffs turns agent coordination into a decentralized message bus:

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Agent A â”‚         â”‚ Agent B â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚                   â”‚
       â”‚  memory_remember  â”‚  memory_recall
       â”‚  handoff_send     â”‚  handoff_get
       â”‚                   â”‚
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
  â”‚       Memory Palace         â”‚
  â”‚    (persistent memory +     â”‚
  â”‚     message bus)            â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚                   â”‚
       â”‚  memory_recall    â”‚  memory_remember
       â”‚  handoff_get      â”‚  handoff_send
       â”‚                   â”‚
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
  â”‚ Agent C â”‚         â”‚ Agent D â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**No controller.** Each agent reads and writes to shared memory. Each agent can leave targeted handoff messages for specific other agents. They coordinate through the data store, not through a supervisor.

Each worker can be a *different model*. Cheap local model for routine tasks, Claude for complex reasoning, specialized fine-tuned model for domain work â€” all sharing the same memory, all passing messages through the same bus. No single model needs to hold the whole picture.

**Current implementation:** Agents poll for handoffs via `handoff_get`. Push notifications via LISTEN/NOTIFY are planned but not yet implemented.

## Backends

Memory Palace currently ships with two backends:

```
SQLite (personal)     PostgreSQL (team/enterprise)
  Zero config            Concurrent access
  Single file            pgvector search
  No dependencies        Scales with infra
       â””â”€â”€â”€â”€ Same MCP API â”€â”€â”€â”€â”˜
```

| Tier | Backend | Concurrent Agents | Use Case | Status |
|------|---------|-------------------|----------|--------|
| Personal | SQLite | 1â€“10 | Individual developer, local AI instances | âœ… Shipping |
| Team | PostgreSQL + pgvector | 10â€“100 | Dev team sharing AI memory | ğŸ”§ Code complete |
| Department | PostgreSQL + read replicas | 100â€“500 | Cross-team knowledge sharing | ğŸ“‹ Planned |
| Enterprise | PostgreSQL cluster | 500â€“10,000+ | Full agent swarm orchestration | ğŸ“‹ Planned |

**Legend:**
- âœ… Shipping â€” Built, tested, in daily use
- ğŸ”§ Code complete â€” Implementation exists, needs production validation  
- ğŸ“‹ Planned â€” Architecture defined, implementation not started

SQLite is the default for zero-config setup â€” no database server needed, just a file. PostgreSQL is a config change away, no code changes required.

### Why PostgreSQL for Scale

SQLite is perfect for single-user local use. It's fast, zero-config, and file-based. But SQLite has a write lock â€” one writer at a time. That's fine for one person. It's not fine for 100+ concurrent agents.

PostgreSQL with pgvector provides:

- **MVCC (Multi-Version Concurrency Control)** â€” Every agent reads and writes without blocking others
- **pgvector** â€” Native vector similarity search with indexing at database scale
- **Connection pooling** â€” SQLAlchemy's QueuePool handles connection reuse (external pooling like PgBouncer would help at higher scale)
- **Replication** â€” Read replicas for recall-heavy workloads (architecture defined, not yet implemented)

Switching from SQLite to PostgreSQL is a one-line config change:

```json
{
  "database": {
    "type": "postgres",
    "url": "postgresql://user:pass@localhost/memory_palace"
  }
}
```

No client changes. No data migration tool needed. The MCP API is identical.

### Roadmap: Enterprise Features

The following are architected but **not yet implemented**:

- **Schema-based tenant isolation** â€” Each department gets its own PostgreSQL schema for data isolation
- **PgBouncer integration** â€” External connection pooling for thousands of concurrent agents
- **LISTEN/NOTIFY** â€” Push-based handoff delivery instead of polling
- **Read replicas** â€” Separate read scaling from write path

### Air-Gapped & Sovereign Deployment

Because Memory Palace runs entirely on local infrastructure with local models (Ollama), it can be deployed in air-gapped environments. No cloud APIs required. No data leaves the network.

This is critical for:
- Government and defense applications
- Healthcare (HIPAA compliance)
- Financial services (data residency requirements)
- Any organization with strict data sovereignty policies

## Design Principles

1. **Open Protocol** â€” MCP is a standard. Any compliant client works. No proprietary lock-in.
2. **Local-First** â€” All processing happens on your hardware by default. Cloud is optional, not required.
3. **Data Ownership** â€” Your memories are in a standard database you can query, export, backup, and audit.
4. **Backend Agnostic** â€” The MCP API stays the same whether you're running SQLite or a PostgreSQL cluster.
5. **Model Agnostic** â€” Any AI that speaks MCP gets persistent memory. Switch models freely.
